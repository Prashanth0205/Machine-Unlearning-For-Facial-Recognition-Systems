{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn import linear_model, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the input data\n",
    "train_metadata_path = \"custom_korean_family_dataset_resolution_128\\custom_train_dataset.csv\"\n",
    "train_data_path = \"custom_korean_family_dataset_resolution_128\\\\train_images\"\n",
    "\n",
    "test_metadata_path = \"custom_korean_family_dataset_resolution_128\\custom_val_dataset.csv\"\n",
    "test_data_path = \"custom_korean_family_dataset_resolution_128\\\\val_images\"\n",
    "\n",
    "unseen_metadata_path = \"custom_korean_family_dataset_resolution_128\\custom_test_dataset.csv\"\n",
    "unseen_data_path = \"custom_korean_family_dataset_resolution_128\\\\test_images\"\n",
    "\n",
    "train_metadata = pd.read_csv(train_metadata_path)\n",
    "test_metadata = pd.read_csv(test_metadata_path)\n",
    "unseen_metadata = pd.read_csv(unseen_metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, metadata, img_dir, transform = None, forget = False, retain = False):\n",
    "        self.metadata = metadata\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Processing the metadata \n",
    "        image_age_list = []\n",
    "        for _, row in metadata.iterrows():\n",
    "            img_path = row['image_path']\n",
    "            age = row['age_class']\n",
    "            image_age_list.append([img_path, age])\n",
    "        \n",
    "        self.image_age_list = image_age_list\n",
    "        self.age_to_label = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3, \"e\": 4, \"f\": 5, \"g\": 6, \"h\": 7}\n",
    "\n",
    "        if forget:\n",
    "            self.image_age_list = self.image_age_list[:1500]\n",
    "        if retain:\n",
    "            self.image_age_list = self.image_age_list[1500:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_age_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, age = self.image_age_list[index]\n",
    "        img = Image.open(os.path.join(self.img_dir, img_path))\n",
    "        label = self.age_to_label[age]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = Dataset(train_metadata, train_data_path, train_transform)\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "test_data = Dataset(test_metadata, test_data_path, transform)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "unseen_data = Dataset(unseen_metadata, unseen_data_path, transform)\n",
    "unseen_dataloader = DataLoader(unseen_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training images: 10025\n",
      "Number of Testing images: 1539\n",
      "Number of Unseen images: 1504\n"
     ]
    }
   ],
   "source": [
    "# Train, test and unseen data split\n",
    "print(f\"Number of Training images: {train_data.__len__()}\")\n",
    "print(f\"Number of Testing images: {test_data.__len__()}\")\n",
    "print(f\"Number of Unseen images: {unseen_data.__len__()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_age = {\n",
    "    0: \"0-6 years old\",\n",
    "    1: \"7-12 years old\",\n",
    "    2: \"13-19 years old\",\n",
    "    3: \"20-30 years old\",\n",
    "    4: \"31-45 years old\",\n",
    "    5: \"46-55 years old\",\n",
    "    6: \"56-66 years old\",\n",
    "    7: \"67-80 years old\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShardSliceTrainer:\n",
    "    def __init__(self, shards, slices, train_loaders_lists, test_loader, label_to_age):\n",
    "        self.shards = shards\n",
    "        self.slices = slices\n",
    "        self.train_loaders_lists = train_loaders_lists\n",
    "        self.test_loader = test_loader\n",
    "        self.label_to_age = label_to_age\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device)  # Move criterion to GPU\n",
    "        for _ in range(shards):\n",
    "            model = models.resnet18(pretrained=True)\n",
    "            # Freeze all layers except the final few layers\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in model.layer4.parameters():\n",
    "                param.requires_grad = True\n",
    "            model.fc = nn.Linear(model.fc.in_features, len(label_to_age))\n",
    "            model = model.to(self.device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            self.models.append(model)\n",
    "            self.optimizers.append(optimizer)\n",
    "\n",
    "    def train(self):\n",
    "        for shard_idx, model in enumerate(self.models):\n",
    "            print(f\"Training Shard {shard_idx+1}/{self.shards}\")\n",
    "            train_loaders = self.train_loaders_lists[shard_idx]\n",
    "\n",
    "            for slice_idx, loaders in enumerate(train_loaders):\n",
    "                print(f\"Shard: {shard_idx + 1} | Training Slice {slice_idx+1}/{self.slices}\")\n",
    "                running_loss = 0.0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "\n",
    "                # Concatenate datasets within the current shard up to the current slice\n",
    "                if slice_idx == 0:\n",
    "                    cumulative_dataset = loaders.dataset\n",
    "                else:\n",
    "                    cumulative_dataset = torch.utils.data.ConcatDataset([cumulative_dataset, loaders.dataset])\n",
    "\n",
    "                # Create a DataLoader from the concatenated dataset\n",
    "                cumulative_loader = DataLoader(cumulative_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "                # Train the model for dynamic number of epochs on cumulative data\n",
    "                for epoch in tqdm(range(slice_idx + 1), desc=f\"Epoch {slice_idx+1}/{self.slices}\"):\n",
    "                    for data in cumulative_loader:\n",
    "                        inputs = data[0].to(self.device)\n",
    "                        labels = data[1].to(self.device)\n",
    "                        optimizer = self.optimizers[shard_idx]\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(inputs)\n",
    "                        loss = self.criterion(outputs, labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        running_loss += loss.item()\n",
    "\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "                accuracy = 100 * correct / total\n",
    "                print(f\"Shard {shard_idx}, Slice {slice_idx+1}, Loss: {running_loss/(total):.2f}, Accuracy: {accuracy:.2f}%\")\n",
    "            print()\n",
    "    \n",
    "    def test(self):\n",
    "        print(\"Testing...\")\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Iterate through each test data point\n",
    "        with torch.no_grad():\n",
    "            for data in self.test_loader:\n",
    "                inputs, labels = data[0].to(self.device), data[1].to(self.device)\n",
    "                ensemble_outputs = torch.zeros(inputs.size(0), len(self.label_to_age)).to(self.device)  # Initialize ensemble outputs\n",
    "                ensemble_loss = 0.0\n",
    "\n",
    "                # Aggregate predictions and losses from all models in the ensemble\n",
    "                for model in self.models:\n",
    "                    outputs = model(inputs)\n",
    "                    ensemble_outputs += outputs\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    ensemble_loss += loss.item()\n",
    "\n",
    "                # Choose prediction with maximum probability in ensemble outputs\n",
    "                _, predicted = torch.max(ensemble_outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Accumulate loss from each model and calculate average loss\n",
    "            total_loss += ensemble_loss / len(self.models)\n",
    "\n",
    "        # Calculate accuracy and average loss over all batches\n",
    "        accuracy = 100 * correct / total\n",
    "        loss = total_loss / len(self.test_loader)  # Calculate average loss over all batches\n",
    "        print(f\"Ensemble Test Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Ensemble Test Loss: {loss:.2f}\")\n",
    "                    \n",
    "                        \n",
    "    def get_slice_index(self, image_index, shards, slices, dataset_length):\n",
    "        samples_per_shard = dataset_length // shards\n",
    "        samples_per_slice = samples_per_shard // slices\n",
    "\n",
    "        # Iterate through each shard\n",
    "        for shard_idx in range(shards):\n",
    "            start_idx = shard_idx * samples_per_shard\n",
    "            end_idx = (shard_idx + 1) * samples_per_shard\n",
    "\n",
    "            # Check if the image index falls within the range of samples for the current shard\n",
    "            if start_idx <= image_index < end_idx:\n",
    "                # Calculate the relative index within the shard\n",
    "                relative_index = image_index - start_idx\n",
    "\n",
    "                # Calculate the slice index\n",
    "                slice_index = relative_index // samples_per_slice\n",
    "                return shard_idx, slice_index\n",
    "\n",
    "        return None \n",
    "\n",
    "    def find_slice_idx(self, train_loader, image):\n",
    "        slice_idx_list = 0\n",
    "        for i in range(len(train_loader.dataset)):\n",
    "            if torch.equal(image, train_loader.dataset[i][0]):\n",
    "                slice_idx_list = i\n",
    "                break\n",
    "        return slice_idx_list\n",
    "    \n",
    "    def unlearn(self, dataloader, image_index):\n",
    "        # Get shard and slice index for the image\n",
    "        shard_idx, slice_idx = self.get_slice_index(image_index, self.shards, self.slices, len(dataloader.dataset))\n",
    "        \n",
    "        # Retrieve the model and optimizer for the identified shard\n",
    "        model = self.models[shard_idx]\n",
    "        optimizer = self.optimizers[shard_idx]\n",
    "        \n",
    "        image, label = dataloader.dataset[image_index]  # Extract image and label from dataloader\n",
    "\n",
    "        # Gather data from the identified slice, excluding the image to be removed\n",
    "        shard_slice_data_loader = self.train_loaders_lists[shard_idx][slice_idx]\n",
    "        updated_data = []\n",
    "        for data, target in shard_slice_data_loader:\n",
    "            if not torch.equal(data, image):  # Exclude the image to be removed\n",
    "                updated_data.append((data, target))\n",
    "        \n",
    "        # Create a new DataLoader or Subset for the retraining data\n",
    "        retraining_data_loader = DataLoader(updated_data, batch_size=1, shuffle=True)\n",
    "        \n",
    "        # Retrain the model using the retraining data\n",
    "        for epoch in range(slice_idx + 1):\n",
    "            for batch in retraining_data_loader:\n",
    "                inputs, labels = batch[0], batch[1]\n",
    "                inputs = inputs[0].to(self.device)\n",
    "                labels = labels[0].to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Optionally return the updated model\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "shards = 4\n",
    "slices = 10\n",
    "\n",
    "shard_data_loaders_lists = []\n",
    "\n",
    "for shard_idx in range(shards):\n",
    "    start_idx = (len(train_dataloader.dataset) // shards) * shard_idx\n",
    "    end_idx = (len(train_dataloader.dataset) // shards) * (shard_idx + 1)\n",
    "\n",
    "    shard_subset = Subset(train_dataloader.dataset, range(start_idx, end_idx))\n",
    "\n",
    "    samples_per_slice = len(shard_subset) // slices\n",
    "\n",
    "    shard_slice_data_loaders = []\n",
    "\n",
    "    for slice_idx in range(slices):\n",
    "        slice_start_idx = slice_idx * samples_per_slice\n",
    "        slice_end_idx = (slice_idx + 1) * samples_per_slice if slice_idx < slices - 1 else len(shard_subset)\n",
    "\n",
    "        slice_subset = Subset(shard_subset, range(slice_start_idx, slice_end_idx))\n",
    "\n",
    "        slice_data_loader = DataLoader(slice_subset, batch_size=1, shuffle=True)\n",
    "\n",
    "        shard_slice_data_loaders.append(slice_data_loader)\n",
    "\n",
    "    shard_data_loaders_lists.append(shard_slice_data_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shard 1/4\n",
      "Shard: 1 | Training Slice 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1/1 [00:02<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0, Slice 1, Loss: 2.27, Accuracy: 17.60%\n",
      "Shard: 1 | Training Slice 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 2/2 [00:10<00:00,  5.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0, Slice 2, Loss: 2.02, Accuracy: 26.20%\n",
      "Shard: 1 | Training Slice 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 3/3 [00:23<00:00,  7.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0, Slice 3, Loss: 1.37, Accuracy: 48.53%\n",
      "Shard: 1 | Training Slice 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 4/4 [00:40<00:00, 10.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0, Slice 4, Loss: 0.63, Accuracy: 78.10%\n",
      "Shard: 1 | Training Slice 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 5/5 [00:52<00:00, 10.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0, Slice 5, Loss: 0.22, Accuracy: 93.94%\n",
      "Shard: 1 | Training Slice 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6/6 [01:20<00:00, 13.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0, Slice 6, Loss: 0.22, Accuracy: 94.26%\n",
      "Shard: 1 | Training Slice 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 7/7 [01:41<00:00, 14.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0, Slice 7, Loss: 0.14, Accuracy: 96.40%\n",
      "Shard: 1 | Training Slice 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 8/8 [02:26<00:00, 18.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0, Slice 8, Loss: 0.12, Accuracy: 96.63%\n",
      "Shard: 1 | Training Slice 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 9/9 [03:45<00:00, 25.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0, Slice 9, Loss: 0.08, Accuracy: 98.04%\n",
      "Shard: 1 | Training Slice 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 10/10 [04:56<00:00, 29.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0, Slice 10, Loss: 0.08, Accuracy: 97.84%\n",
      "\n",
      "Training Shard 2/4\n",
      "Shard: 2 | Training Slice 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1/1 [00:03<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 1, Slice 1, Loss: 2.43, Accuracy: 14.40%\n",
      "Shard: 2 | Training Slice 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 2/2 [00:11<00:00,  5.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 1, Slice 2, Loss: 1.91, Accuracy: 27.60%\n",
      "Shard: 2 | Training Slice 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 3/3 [00:27<00:00,  9.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 1, Slice 3, Loss: 1.06, Accuracy: 61.91%\n",
      "Shard: 2 | Training Slice 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 4/4 [00:39<00:00,  9.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 1, Slice 4, Loss: 0.34, Accuracy: 89.58%\n",
      "Shard: 2 | Training Slice 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 5/5 [01:07<00:00, 13.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 1, Slice 5, Loss: 0.24, Accuracy: 93.57%\n",
      "Shard: 2 | Training Slice 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6/6 [01:45<00:00, 17.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 1, Slice 6, Loss: 0.21, Accuracy: 94.21%\n",
      "Shard: 2 | Training Slice 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 7/7 [01:58<00:00, 16.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 1, Slice 7, Loss: 0.14, Accuracy: 95.97%\n",
      "Shard: 2 | Training Slice 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 8/8 [02:18<00:00, 17.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 1, Slice 8, Loss: 0.09, Accuracy: 97.58%\n",
      "Shard: 2 | Training Slice 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 9/9 [02:53<00:00, 19.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 1, Slice 9, Loss: 0.07, Accuracy: 98.13%\n",
      "Shard: 2 | Training Slice 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 10/10 [03:28<00:00, 20.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 1, Slice 10, Loss: 0.06, Accuracy: 98.42%\n",
      "\n",
      "Training Shard 3/4\n",
      "Shard: 3 | Training Slice 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 2, Slice 1, Loss: 2.27, Accuracy: 15.60%\n",
      "Shard: 3 | Training Slice 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 2/2 [00:08<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 2, Slice 2, Loss: 1.88, Accuracy: 28.60%\n",
      "Shard: 3 | Training Slice 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 3/3 [00:20<00:00,  6.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 2, Slice 3, Loss: 1.20, Accuracy: 54.49%\n",
      "Shard: 3 | Training Slice 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 4/4 [00:33<00:00,  8.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 2, Slice 4, Loss: 0.56, Accuracy: 80.55%\n",
      "Shard: 3 | Training Slice 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 5/5 [00:54<00:00, 10.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 2, Slice 5, Loss: 0.30, Accuracy: 91.60%\n",
      "Shard: 3 | Training Slice 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6/6 [01:12<00:00, 12.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 2, Slice 6, Loss: 0.15, Accuracy: 95.99%\n",
      "Shard: 3 | Training Slice 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 7/7 [01:40<00:00, 14.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 2, Slice 7, Loss: 0.11, Accuracy: 97.40%\n",
      "Shard: 3 | Training Slice 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 8/8 [02:16<00:00, 17.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 2, Slice 8, Loss: 0.08, Accuracy: 98.05%\n",
      "Shard: 3 | Training Slice 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 9/9 [02:46<00:00, 18.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 2, Slice 9, Loss: 0.08, Accuracy: 97.94%\n",
      "Shard: 3 | Training Slice 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 10/10 [03:26<00:00, 20.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 2, Slice 10, Loss: 0.04, Accuracy: 99.04%\n",
      "\n",
      "Training Shard 4/4\n",
      "Shard: 4 | Training Slice 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1/1 [00:02<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 3, Slice 1, Loss: 2.10, Accuracy: 19.60%\n",
      "Shard: 4 | Training Slice 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 2/2 [00:08<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 3, Slice 2, Loss: 1.65, Accuracy: 38.00%\n",
      "Shard: 4 | Training Slice 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 3/3 [00:18<00:00,  6.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 3, Slice 3, Loss: 0.92, Accuracy: 65.82%\n",
      "Shard: 4 | Training Slice 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 4/4 [00:34<00:00,  8.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 3, Slice 4, Loss: 0.45, Accuracy: 85.12%\n",
      "Shard: 4 | Training Slice 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 5/5 [00:51<00:00, 10.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 3, Slice 5, Loss: 0.21, Accuracy: 94.35%\n",
      "Shard: 4 | Training Slice 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6/6 [01:12<00:00, 12.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 3, Slice 6, Loss: 0.14, Accuracy: 96.26%\n",
      "Shard: 4 | Training Slice 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 7/7 [01:47<00:00, 15.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 3, Slice 7, Loss: 0.11, Accuracy: 97.18%\n",
      "Shard: 4 | Training Slice 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 8/8 [02:09<00:00, 16.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 3, Slice 8, Loss: 0.07, Accuracy: 98.24%\n",
      "Shard: 4 | Training Slice 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 9/9 [02:48<00:00, 18.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 3, Slice 9, Loss: 0.07, Accuracy: 98.17%\n",
      "Shard: 4 | Training Slice 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 10/10 [03:46<00:00, 22.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 3, Slice 10, Loss: 0.06, Accuracy: 98.67%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "shard_slice_trainer = ShardSliceTrainer(shards, slices, shard_data_loaders_lists, test_dataloader, label_to_age)\n",
    "shard_slice_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "Ensemble Test Accuracy: 32.29%\n",
      "Ensemble Test Loss: 3.28\n"
     ]
    }
   ],
   "source": [
    "shard_slice_trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1529, 0.1529, 0.1333,  ..., 0.2471, 0.2392, 0.2510],\n",
       "         [0.1255, 0.1373, 0.1490,  ..., 0.2078, 0.2784, 0.2353],\n",
       "         [0.1059, 0.0902, 0.1059,  ..., 0.3020, 0.2549, 0.2392],\n",
       "         ...,\n",
       "         [0.8118, 0.8196, 0.8314,  ..., 0.8627, 0.7961, 0.7804],\n",
       "         [0.7922, 0.8078, 0.8118,  ..., 0.9137, 0.8078, 0.8157],\n",
       "         [0.7922, 0.8039, 0.8078,  ..., 0.8784, 0.8549, 0.7686]],\n",
       "\n",
       "        [[0.1176, 0.1176, 0.1098,  ..., 0.2627, 0.2549, 0.2667],\n",
       "         [0.1020, 0.1137, 0.1333,  ..., 0.2235, 0.2941, 0.2510],\n",
       "         [0.1020, 0.0863, 0.1020,  ..., 0.3176, 0.2706, 0.2549],\n",
       "         ...,\n",
       "         [0.2588, 0.2667, 0.2784,  ..., 0.7647, 0.6980, 0.6824],\n",
       "         [0.2706, 0.2784, 0.2784,  ..., 0.8078, 0.7020, 0.7098],\n",
       "         [0.2863, 0.2902, 0.2745,  ..., 0.7725, 0.7451, 0.6588]],\n",
       "\n",
       "        [[0.1216, 0.1216, 0.1176,  ..., 0.2588, 0.2510, 0.2627],\n",
       "         [0.1020, 0.1137, 0.1294,  ..., 0.2196, 0.2980, 0.2471],\n",
       "         [0.0863, 0.0706, 0.0863,  ..., 0.3216, 0.2824, 0.2588],\n",
       "         ...,\n",
       "         [0.0784, 0.0863, 0.0980,  ..., 0.5529, 0.4863, 0.4627],\n",
       "         [0.1020, 0.1020, 0.0902,  ..., 0.6000, 0.4941, 0.5020],\n",
       "         [0.1216, 0.1176, 0.0863,  ..., 0.5647, 0.5490, 0.4627]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unlearning\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "total_samples = len(train_dataloader.dataset)\n",
    "num_images = 10\n",
    "\n",
    "indices = list(range(total_samples))\n",
    "\n",
    "torch.manual_seed(42)  \n",
    "random_indices = torch.randperm(total_samples)\n",
    "\n",
    "idx = random_indices[0]\n",
    "\n",
    "image, label = train_dataloader.dataset[idx]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5992"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shard_slice_trainer.find_slice_idx(train_dataloader, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shard_slice_trainer.unlearn(train_dataloader, 5992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARD = 2; SLICE = 5\n",
    "\n",
    "'''\n",
    "Training Shard 1/2\n",
    "Shard: 1 | Training Slice 1/5\n",
    "Epoch 1/5: 100%|██████████| 1/1 [00:11<00:00, 11.38s/it]\n",
    "Shard 0, Slice 1, Loss: 2.18, Accuracy: 20.66%\n",
    "Shard: 1 | Training Slice 2/5\n",
    "Epoch 2/5: 100%|██████████| 2/2 [00:34<00:00, 17.29s/it]\n",
    "Shard 0, Slice 2, Loss: 1.70, Accuracy: 33.98%\n",
    "Shard: 1 | Training Slice 3/5\n",
    "Epoch 3/5: 100%|██████████| 3/3 [01:18<00:00, 26.14s/it]\n",
    "Shard 0, Slice 3, Loss: 1.13, Accuracy: 54.60%\n",
    "Shard: 1 | Training Slice 4/5\n",
    "Epoch 4/5: 100%|██████████| 4/4 [02:22<00:00, 35.72s/it]\n",
    "Shard 0, Slice 4, Loss: 0.53, Accuracy: 80.06%\n",
    "Shard: 1 | Training Slice 5/5\n",
    "Epoch 5/5: 100%|██████████| 5/5 [03:35<00:00, 43.09s/it]\n",
    "Shard 0, Slice 5, Loss: 0.21, Accuracy: 93.29%\n",
    "\n",
    "Training Shard 2/2\n",
    "Shard: 2 | Training Slice 1/5\n",
    "Epoch 1/5: 100%|██████████| 1/1 [00:10<00:00, 10.63s/it]\n",
    "Shard 1, Slice 1, Loss: 2.12, Accuracy: 20.56%\n",
    "Shard: 2 | Training Slice 2/5\n",
    "Epoch 2/5: 100%|██████████| 2/2 [00:39<00:00, 19.69s/it]\n",
    "Shard 1, Slice 2, Loss: 1.44, Accuracy: 42.44%\n",
    "Shard: 2 | Training Slice 3/5\n",
    "Epoch 3/5: 100%|██████████| 3/3 [01:22<00:00, 27.52s/it]\n",
    "Shard 1, Slice 3, Loss: 0.86, Accuracy: 67.22%\n",
    "Shard: 2 | Training Slice 4/5\n",
    "Epoch 4/5: 100%|██████████| 4/4 [02:21<00:00, 35.43s/it]\n",
    "Shard 1, Slice 4, Loss: 0.41, Accuracy: 86.18%\n",
    "Shard: 2 | Training Slice 5/5\n",
    "Epoch 5/5: 100%|██████████| 5/5 [03:38<00:00, 43.72s/it]\n",
    "Shard 1, Slice 5, Loss: 0.20, Accuracy: 93.52%\n",
    "\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Speechbrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
